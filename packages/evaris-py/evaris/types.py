"""Core type definitions for Evaris evaluation framework."""

from abc import ABC, abstractmethod
from collections.abc import Awaitable
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, Literal, Optional, Union

from pydantic import BaseModel, ConfigDict, Field

if TYPE_CHECKING:
    from evaris.baselines import BaselineComparisonReport

# Multi-modal input/output types for agent interface
# Supports text, images, audio, video, structured data
MultiModalInput = Union[
    str,  # Text input
    dict[str, Any],  # Structured input (JSON-like)
    list[Union[str, Path]],  # Multiple inputs (text + files)
    Path,  # File input (image, audio, video, etc.)
]

MultiModalOutput = Union[
    str,  # Text output
    dict[str, Any],  # Structured output (JSON-like)
    list[Any],  # Multiple outputs
]


class Golden(BaseModel):
    """Static test data without actual outputs (ground truth).

    This can usually be data through which the model is finetuned,
    tested or evaluated against.

    Goldens provide a strong base to compare/evaluate against actual
    output.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    input: Any = Field(..., description="Input to the agent")
    expected: Optional[Any] = Field(None, description="Expected/golden output")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metadata for the golden"
    )


class TestCase(BaseModel):
    """A complete test case with actual output for evaluation."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    input: Any = Field(..., description="Input to the agent")
    expected: Optional[Any] = Field(None, description="Expected output from the agent")
    actual_output: Optional[Any] = Field(
        None, description="Pre-computed actual output (for offline evaluation)"
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metadata for the test case"
    )

    @classmethod
    def from_golden(cls, golden: Golden, actual_output: Any = None) -> "TestCase":
        """Create a TestCase from a Golden and an actual output.

        Args:
            golden: The golden test data (input + expected)
            actual_output: The actual output generated by the agent (optional)

        Returns:
            TestCase: A complete test case ready for evaluation

        Note:
            Metadata is shared with Golden (not copied) for performance.
            Code that modifies TestCase.metadata must create a copy first
            to avoid polluting the original Golden's metadata.
        """
        return cls(
            input=golden.input,
            expected=golden.expected,
            actual_output=actual_output,
            metadata=golden.metadata,  # Share dict for performance
        )


class ReasoningStep(BaseModel):
    """A single step in the reasoning trace for metric evaluation.

    This model captures intermediate values and logic during metric evaluation,
    providing transparency into how scores are calculated.

    Example:
        >>> step = ReasoningStep(
        ...     step_number=1,
        ...     operation="text_normalization",
        ...     description="Normalizing text before comparison",
        ...     inputs={"text": "Paris"},
        ...     outputs={"normalized": "paris"}
        ... )
    """

    step_number: int = Field(..., description="Sequential step number (1-indexed)")
    operation: str = Field(..., description="Type of operation performed")
    description: str = Field(..., description="Human-readable description of this step")
    inputs: dict[str, Any] = Field(..., description="Input values for this step")
    outputs: dict[str, Any] = Field(..., description="Output values from this step")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional step-specific metadata"
    )


class MetricResult(BaseModel):
    """Result from a single metric evaluation.

    Includes optional reasoning fields for transparency into metric calculation.
    """

    name: str = Field(..., description="Metric name")
    score: float = Field(..., ge=0.0, le=1.0, description="Score between 0 and 1")
    passed: bool = Field(..., description="Whether the test passed")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metric-specific data"
    )

    # Reasoning fields (optional, for transparency)
    reasoning: Optional[str] = Field(
        default=None, description="Human-readable reasoning summary explaining the score"
    )
    reasoning_steps: Optional[list[ReasoningStep]] = Field(
        default=None, description="Structured trace of reasoning steps taken during evaluation"
    )
    reasoning_type: Optional[Literal["logic", "llm", "hybrid"]] = Field(
        default=None,
        description=(
            "Type of reasoning used: 'logic' for rule-based, "
            "'llm' for LLM-based, 'hybrid' for both"
        ),
    )


class TestResult(BaseModel):
    """Result from evaluating a single test case."""

    test_case: TestCase = Field(..., description="The test case that was evaluated")
    output: Any = Field(..., description="Actual output from the agent")
    metrics: list[MetricResult] = Field(..., description="Metric results")
    latency_ms: float = Field(..., ge=0.0, description="Execution time in milliseconds")
    error: Optional[str] = Field(None, description="Error message if execution failed")


class EvalResult(BaseModel):
    """Aggregated results from an evaluation run."""

    name: str = Field(..., description="Evaluation name")
    total: int = Field(..., ge=0, description="Total number of test cases")
    passed: int = Field(..., ge=0, description="Number of passed test cases")
    failed: int = Field(..., ge=0, description="Number of failed test cases")
    accuracy: float = Field(..., ge=0.0, le=1.0, description="Overall accuracy")
    avg_latency_ms: float = Field(..., ge=0.0, description="Average latency in milliseconds")
    results: list[TestResult] = Field(..., description="Individual test results")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional evaluation metadata"
    )
    # Baseline comparison fields
    baseline_results: Optional[dict[str, "EvalResult"]] = Field(
        None, description="Evaluation results for each baseline (baseline_name -> EvalResult)"
    )
    baseline_comparison: Optional["BaselineComparisonReport"] = Field(
        None, description="Comparison report between agent and baselines"
    )

    def __str__(self) -> str:
        """Human-readable summary of evaluation results."""
        return (
            f"Evaluation: {self.name}\n"
            f"Total: {self.total} | Passed: {self.passed} | Failed: {self.failed}\n"
            f"Accuracy: {self.accuracy:.2%} | Avg Latency: {self.avg_latency_ms:.2f}ms"
        )


class BaseMetric(ABC):
    """Abstract base class for all metrics.

    All metrics must implement the async a_measure() method for async evaluation.
    Optionally, metrics can provide a measure() method for synchronous evaluation.
    """

    @abstractmethod
    async def a_measure(self, test_case: TestCase, actual_output: Any) -> MetricResult:
        """Asynchronously evaluate a test case.

        Args:
            test_case: The test case to evaluate
            actual_output: The actual output from the agent

        Returns:
            MetricResult: The evaluation result with score and pass/fail status
        """
        pass

    def measure(self, test_case: TestCase, actual_output: Any) -> MetricResult:
        """Synchronously evaluate a test case (default implementation).

        This is a convenience wrapper around a_measure() for synchronous use.
        Metrics should override this method with a sync implementation for better
        performance, as this default creates a new event loop for each call.

        WARNING: This method uses asyncio.run() which creates a new event loop.
        - Performance overhead when called repeatedly
        - Cannot be called from async context (use a_measure() instead)
        - For best performance, metrics should implement a native sync version

        Args:
            test_case: The test case to evaluate
            actual_output: The actual output from the agent (optional, for legacy support)

        Returns:
            MetricResult: The evaluation result

        Raises:
            RuntimeError: If called from an async context
        """
        import asyncio

        # Check if we're in an async context (would fail with asyncio.run)
        try:
            asyncio.get_running_loop()
            # If we get here, there's a running loop - can't use asyncio.run()
            raise RuntimeError(
                "measure() cannot be called from async context. "
                "Use a_measure() instead, or implement a native sync measure() method."
            )
        except RuntimeError as e:
            # If error message contains "no running event loop", we're good to proceed
            if "no running event loop" not in str(e):
                # Re-raise if it's our custom error or other RuntimeError
                raise

        return asyncio.run(self.a_measure(test_case, actual_output))

    def score(self, test_case: TestCase, actual_output: Any) -> MetricResult:
        """Legacy synchronous evaluation method (optional).

        This is the legacy interface that takes actual_output separately.
        Modern metrics should use measure() or a_measure() instead.

        The default implementation raises NotImplementedError. Metrics that
        support the legacy interface should override this method.

        Args:
            test_case: The test case to evaluate
            actual_output: The actual output from the agent

        Returns:
            MetricResult: The evaluation result

        Raises:
            NotImplementedError: If metric doesn't implement legacy score() interface
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} doesn't implement the legacy score() method. "
            "Use measure() or a_measure() instead."
        )


# Type aliases for better readability
AgentFunction = Callable[[Any], Any]
AsyncAgentFunction = Callable[[Any], Awaitable[Any]]
DatasetInput = Union[list[dict[str, Any]], list[TestCase], list[Golden]]
