"""Core type definitions for Evaris evaluation framework.

This module defines the fundamental data structures used throughout Evaris:
- Golden: Ground truth test data
- TestCase: Complete test case with actual output
- MetricResult: Single metric evaluation result
- TestResult: Complete test case result
- EvalResult: Aggregated evaluation results
"""

from collections.abc import Awaitable
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, Literal, Optional, Union

from pydantic import BaseModel, ConfigDict, Field

if TYPE_CHECKING:
    from evaris.baselines import BaselineComparisonReport


# Multi-modal input/output types for agent interface
# Supports text, images, audio, video, structured data
MultiModalInput = Union[
    str,  # Text input
    dict[str, Any],  # Structured input (JSON-like)
    list[Union[str, Path]],  # Multiple inputs (text + files)
    Path,  # File input (image, audio, video, etc.)
]

MultiModalOutput = Union[
    str,  # Text output
    dict[str, Any],  # Structured output (JSON-like)
    list[Any],  # Multiple outputs
]


class Golden(BaseModel):
    """Static test data without actual outputs (ground truth).

    This can usually be data through which the model is finetuned,
    tested or evaluated against.

    Goldens provide a strong base to compare/evaluate against actual
    output.

    Attributes:
        input: The input to be sent to the agent
        expected: The expected/golden output (optional)
        metadata: Additional metadata for the golden

    Example:
        >>> golden = Golden(
        ...     input="What is the capital of France?",
        ...     expected="Paris",
        ...     metadata={"category": "geography"}
        ... )
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    input: Any = Field(..., description="Input to the agent")
    expected: Optional[Any] = Field(None, description="Expected/golden output")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metadata for the golden"
    )


class TestCase(BaseModel):
    """A complete test case with actual output for evaluation.

    TestCase extends Golden by adding the actual_output field, which
    stores the agent's response. This is used for evaluation.

    Attributes:
        input: The input to be sent to the agent
        expected: The expected/golden output (optional)
        actual_output: The agent's actual response (optional, for offline eval)
        metadata: Additional metadata for the test case

    Example:
        >>> test_case = TestCase(
        ...     input="What is 2+2?",
        ...     expected="4",
        ...     actual_output="4",
        ...     metadata={"difficulty": "easy"}
        ... )
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    input: Any = Field(..., description="Input to the agent")
    expected: Optional[Any] = Field(None, description="Expected output from the agent")
    actual_output: Optional[Any] = Field(
        None, description="Pre-computed actual output (for offline evaluation)"
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metadata for the test case"
    )

    @classmethod
    def from_golden(cls, golden: Golden, actual_output: Any = None) -> "TestCase":
        """Create a TestCase from a Golden and an actual output.

        Args:
            golden: The golden test data (input + expected)
            actual_output: The actual output generated by the agent (optional)

        Returns:
            TestCase: A complete test case ready for evaluation

        Note:
            Metadata is copied from Golden to prevent accidental mutation
            of the original Golden's metadata.
        """
        return cls(
            input=golden.input,
            expected=golden.expected,
            actual_output=actual_output,
            metadata=golden.metadata.copy() if golden.metadata else {},
        )


class ReasoningStep(BaseModel):
    """A single step in the reasoning trace for metric evaluation.

    This model captures intermediate values and logic during metric evaluation,
    providing transparency into how scores are calculated. Essential for
    ABC compliance (explainability requirement).

    Attributes:
        step_number: Sequential step number (1-indexed)
        operation: Type of operation performed
        description: Human-readable description
        inputs: Input values for this step
        outputs: Output values from this step
        metadata: Additional step-specific metadata

    Example:
        >>> step = ReasoningStep(
        ...     step_number=1,
        ...     operation="text_normalization",
        ...     description="Normalizing text before comparison",
        ...     inputs={"text": "Paris"},
        ...     outputs={"normalized": "paris"}
        ... )
    """

    step_number: int = Field(..., description="Sequential step number (1-indexed)")
    operation: str = Field(..., description="Type of operation performed")
    description: str = Field(..., description="Human-readable description of this step")
    inputs: dict[str, Any] = Field(..., description="Input values for this step")
    outputs: dict[str, Any] = Field(..., description="Output values from this step")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional step-specific metadata"
    )


class MetricResult(BaseModel):
    """Result from a single metric evaluation.

    Contains the score, pass/fail status, and optional reasoning for
    transparency into how the metric calculated the score.

    Attributes:
        name: Metric name
        score: Score between 0 and 1
        passed: Whether the test passed
        metadata: Additional metric-specific data
        reasoning: Human-readable reasoning summary
        reasoning_steps: Structured trace of reasoning steps
        reasoning_type: Type of reasoning ('logic', 'llm', 'hybrid')

    Example:
        >>> result = MetricResult(
        ...     name="exact_match",
        ...     score=1.0,
        ...     passed=True,
        ...     reasoning="Exact match after normalization"
        ... )
    """

    name: str = Field(..., description="Metric name")
    score: float = Field(..., ge=0.0, le=1.0, description="Score between 0 and 1")
    passed: bool = Field(..., description="Whether the test passed")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional metric-specific data"
    )

    # Reasoning fields (optional, for transparency)
    reasoning: Optional[str] = Field(
        default=None, description="Human-readable reasoning summary explaining the score"
    )
    reasoning_steps: Optional[list[ReasoningStep]] = Field(
        default=None, description="Structured trace of reasoning steps taken during evaluation"
    )
    reasoning_type: Optional[Literal["logic", "llm", "hybrid"]] = Field(
        default=None,
        description=(
            "Type of reasoning used: 'logic' for rule-based, "
            "'llm' for LLM-based, 'hybrid' for both"
        ),
    )


class TestResult(BaseModel):
    """Result from evaluating a single test case.

    Contains the test case, actual output, all metric results,
    execution latency, and any errors that occurred.

    Attributes:
        test_case: The test case that was evaluated
        output: Actual output from the agent
        metrics: List of metric results
        latency_ms: Execution time in milliseconds
        error: Error message if execution failed
    """

    test_case: TestCase = Field(..., description="The test case that was evaluated")
    output: Any = Field(..., description="Actual output from the agent")
    metrics: list[MetricResult] = Field(..., description="Metric results")
    latency_ms: float = Field(..., ge=0.0, description="Execution time in milliseconds")
    error: Optional[str] = Field(None, description="Error message if execution failed")


class EvalResult(BaseModel):
    """Aggregated results from an evaluation run.

    Contains summary statistics, all individual test results,
    and optional baseline comparison data.

    Attributes:
        name: Evaluation name
        total: Total number of test cases
        passed: Number of passed test cases
        failed: Number of failed test cases
        accuracy: Overall accuracy (passed/total)
        avg_latency_ms: Average latency in milliseconds
        results: Individual test results
        metadata: Additional evaluation metadata
        baseline_results: Results for each baseline
        baseline_comparison: Comparison report
    """

    name: str = Field(..., description="Evaluation name")
    total: int = Field(..., ge=0, description="Total number of test cases")
    passed: int = Field(..., ge=0, description="Number of passed test cases")
    failed: int = Field(..., ge=0, description="Number of failed test cases")
    accuracy: float = Field(..., ge=0.0, le=1.0, description="Overall accuracy")
    avg_latency_ms: float = Field(..., ge=0.0, description="Average latency in milliseconds")
    results: list[TestResult] = Field(..., description="Individual test results")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Additional evaluation metadata"
    )
    # Baseline comparison fields
    baseline_results: Optional[dict[str, "EvalResult"]] = Field(
        None, description="Evaluation results for each baseline (baseline_name -> EvalResult)"
    )
    baseline_comparison: Optional["BaselineComparisonReport"] = Field(
        None, description="Comparison report between agent and baselines"
    )

    def __str__(self) -> str:
        """Human-readable summary of evaluation results."""
        return (
            f"Evaluation: {self.name}\n"
            f"Total: {self.total} | Passed: {self.passed} | Failed: {self.failed}\n"
            f"Accuracy: {self.accuracy:.2%} | Avg Latency: {self.avg_latency_ms:.2f}ms"
        )


# Type aliases for better readability
AgentFunction = Callable[[Any], Any]
AsyncAgentFunction = Callable[[Any], Awaitable[Any]]
DatasetInput = Union[list[dict[str, Any]], list[TestCase], list[Golden]]
